Capstone: Group Project Code - Ctrl Alt Data - Toxicity
Group Members
Name	Student ID
Rohan Saldanha	100976167
Jayanth Hassan Murali	100994668
Ujjwal Vasava	100976100
Akshaya Bhalikha Dhanasekaran	100936892
Bhavanish S Nair	100936855
Mustafa Khaja Masood Khaja	100923081
Grifith Pereira	100991416
# Importing the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
# Importing the dataset
df = pd.read_csv('Toxicity.csv')
df.head()
MDEC-23	MATS2v	ATSC8s	VE3_Dt	CrippenMR	SpMax7_Bhe	SpMin1_Bhs	C1SP2	GATS8e	GATS8s	SpMax5_Bhv	VE3_Dzi	VPC-4	Class
0	60.1757	-0.0231	-0.6667	-167.1241	0.0000	3.4009	2.3109	4	1.0229	1.0575	3.5545	-15.5940	4.1692	NonToxic
1	44.5031	-0.1236	-16.5096	-16.2080	172.2000	3.3611	2.1117	2	1.7155	1.7013	3.6066	-14.3317	2.0821	NonToxic
2	37.5488	0.0662	19.3467	-159.1796	173.4028	3.2705	2.0198	8	0.6992	0.7828	3.6441	-25.4493	2.8730	NonToxic
3	40.5929	0.0714	-9.5672	-21.4416	177.2726	3.2748	2.0191	6	0.9951	1.0298	3.6564	-19.6376	3.0444	NonToxic
4	52.7343	-0.0861	-11.8892	-2.0780	171.1315	3.4094	2.1664	2	0.7363	0.7427	3.5216	-8.2157	2.9469	NonToxic
Exploratory Data Analysis (EDA)
#Checking the length of the dataset
len(df)
171
#Dataset info
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 171 entries, 0 to 170
Data columns (total 14 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   MDEC-23     171 non-null    float64
 1   MATS2v      171 non-null    float64
 2   ATSC8s      171 non-null    float64
 3   VE3_Dt      171 non-null    float64
 4   CrippenMR   171 non-null    float64
 5   SpMax7_Bhe  171 non-null    float64
 6   SpMin1_Bhs  171 non-null    float64
 7   C1SP2       171 non-null    int64  
 8   GATS8e      171 non-null    float64
 9   GATS8s      171 non-null    float64
 10  SpMax5_Bhv  171 non-null    float64
 11  VE3_Dzi     171 non-null    float64
 12  VPC-4       171 non-null    float64
 13  Class       171 non-null    object 
dtypes: float64(12), int64(1), object(1)
memory usage: 18.8+ KB
#Check rows and columns of the dataframe
df.shape
(171, 14)
#Check for duplicates
print(df.duplicated().sum()) 
0
#checking unique values 
df.nunique()
MDEC-23       162
MATS2v        160
ATSC8s        171
VE3_Dt        163
CrippenMR     165
SpMax7_Bhe    157
SpMin1_Bhs    161
C1SP2           8
GATS8e        168
GATS8s        169
SpMax5_Bhv    164
VE3_Dzi       170
VPC-4         166
Class           2
dtype: int64
#Check for null values
print(df.isnull().sum())
MDEC-23       0
MATS2v        0
ATSC8s        0
VE3_Dt        0
CrippenMR     0
SpMax7_Bhe    0
SpMin1_Bhs    0
C1SP2         0
GATS8e        0
GATS8s        0
SpMax5_Bhv    0
VE3_Dzi       0
VPC-4         0
Class         0
dtype: int64
#Total numuber of target values
df['Class'].value_counts()
Class
NonToxic    115
Toxic        56
Name: count, dtype: int64
#Bar plot of the classes
ax = df["Class"].value_counts().plot(kind='bar')
ax.bar_label(ax.containers[0]); 
No description has been provided for this image
df.describe()
MDEC-23	MATS2v	ATSC8s	VE3_Dt	CrippenMR	SpMax7_Bhe	SpMin1_Bhs	C1SP2	GATS8e	GATS8s	SpMax5_Bhv	VE3_Dzi	VPC-4
count	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000	171.000000
mean	39.135014	-0.005382	-6.269639	-32.311274	146.236654	3.220797	2.041885	2.760234	1.090043	1.131815	3.425033	-23.920250	2.568535
std	10.086143	0.081755	30.055317	51.493985	26.937485	0.191441	0.080524	1.551412	0.455048	0.498616	0.199008	42.218482	0.849953
min	7.861200	-0.152700	-120.803600	-167.741600	0.000000	2.247100	1.870900	0.000000	0.207600	0.231900	2.503400	-159.179600	0.000000
25%	32.911850	-0.074050	-19.902350	-16.831150	137.735350	3.157700	1.988400	2.000000	0.840100	0.817600	3.334050	-14.401850	2.183700
50%	39.828400	-0.005500	-5.951200	-8.148000	150.502700	3.260100	2.039900	3.000000	1.022700	1.039100	3.457500	-7.053600	2.647200
75%	45.469500	0.039500	6.988850	-5.928400	162.921750	3.339450	2.078700	4.000000	1.258300	1.357500	3.549150	-4.702950	2.991850
max	65.256000	0.284900	93.508200	-2.067100	180.556500	3.464200	2.347500	8.000000	4.299900	4.223000	3.761600	0.000000	6.185400
#All Columns
df.columns.tolist()
['MDEC-23',
 'MATS2v',
 'ATSC8s',
 'VE3_Dt',
 'CrippenMR',
 'SpMax7_Bhe',
 'SpMin1_Bhs',
 'C1SP2',
 'GATS8e',
 'GATS8s',
 'SpMax5_Bhv',
 'VE3_Dzi',
 'VPC-4',
 'Class']
#Mapping of target class - NonToxic->0 Toxic->1
df["Class"] = df["Class"].map({"NonToxic":0,"Toxic":1})
df
MDEC-23	MATS2v	ATSC8s	VE3_Dt	CrippenMR	SpMax7_Bhe	SpMin1_Bhs	C1SP2	GATS8e	GATS8s	SpMax5_Bhv	VE3_Dzi	VPC-4	Class
0	60.1757	-0.0231	-0.6667	-167.1241	0.0000	3.4009	2.3109	4	1.0229	1.0575	3.5545	-15.5940	4.1692	0
1	44.5031	-0.1236	-16.5096	-16.2080	172.2000	3.3611	2.1117	2	1.7155	1.7013	3.6066	-14.3317	2.0821	0
2	37.5488	0.0662	19.3467	-159.1796	173.4028	3.2705	2.0198	8	0.6992	0.7828	3.6441	-25.4493	2.8730	0
3	40.5929	0.0714	-9.5672	-21.4416	177.2726	3.2748	2.0191	6	0.9951	1.0298	3.6564	-19.6376	3.0444	0
4	52.7343	-0.0861	-11.8892	-2.0780	171.1315	3.4094	2.1664	2	0.7363	0.7427	3.5216	-8.2157	2.9469	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
166	13.6305	-0.1231	-3.1521	-2.9706	71.9807	2.4269	2.0708	1	1.0932	0.7954	2.8103	-4.3924	0.7136	0
167	33.1806	-0.0486	-7.2825	-4.0095	125.0777	3.0040	2.0701	3	1.0687	0.9850	3.2560	-4.4435	1.6616	0
168	28.3570	-0.0726	8.0017	-3.2338	133.7697	3.2468	1.9242	2	1.3249	0.9103	3.2822	-5.5907	1.9339	0
169	26.4286	0.0420	-32.5992	-7.0649	144.0674	3.2089	1.9162	4	1.3001	1.2687	3.2904	-3.5370	2.0526	1
170	9.8829	0.1044	59.0380	-2.5619	53.6910	2.2471	2.0006	1	0.2076	0.2720	2.5034	-3.2052	0.9563	1
171 rows × 14 columns

#Skewness
sns.set_style("darkgrid")
numerical_columns = df.select_dtypes(include=["int64", "float64"]).columns

plt.figure(figsize=(14, len(numerical_columns) * 3))
for idx, feature in enumerate(numerical_columns, 1):
    plt.subplot(len(numerical_columns), 2, idx)
    sns.histplot(df[feature], kde=True)
    plt.title(f"{feature} | Skewness: {round(df[feature].skew(), 2)}")

plt.tight_layout()
plt.show()
No description has been provided for this image
The features in this dataset that have skewness – exactly 0 depicts the symmetrical distribution and the plots with skewness 1 or above 1 is positively or right skewd distribution. In right skewd or positively skewed distribution if the tail is more on the right side, that indicates extremely high values.

#Correlation matrix
# Values close to +1 indicates strong positive correlation, 
#-1 indicates a strong negative correlation 
#and 0 indicates suggests no linear correlation.
corr_matrix = df.corr(method = 'pearson')
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix, 
                 annot=True,
                 linewidths=0.5,
                 fmt=".2f",
                 cmap="YlGnBu")
plt.title('Correlation Heatmap')
plt.show()
No description has been provided for this image
#QQ Plots
data=df
#Create QQ Plots
from statsmodels.graphics.gofplots import qqplot
import pylab
cnt = 1
for col in data.columns:
    print(col)
    qqplot(data[col],line='s')
    pylab.show()
    cnt += 1
MDEC-23
No description has been provided for this image
MATS2v
No description has been provided for this image
ATSC8s
No description has been provided for this image
VE3_Dt
No description has been provided for this image
CrippenMR
No description has been provided for this image
SpMax7_Bhe
No description has been provided for this image
SpMin1_Bhs
No description has been provided for this image
C1SP2
No description has been provided for this image
GATS8e
No description has been provided for this image
GATS8s
No description has been provided for this image
SpMax5_Bhv
No description has been provided for this image
VE3_Dzi
No description has been provided for this image
VPC-4
No description has been provided for this image
Class
No description has been provided for this image
#Normality test - Shapiro-Wilk Test
# p <= alpha(0.05): reject H0, not normal.
# p > alpha(0.05): fail to reject H0, normal.
from scipy.stats import shapiro
cnt=1
data = df
for col in data.columns:
    if (col=='Class'):
        continue
    stat, p = shapiro(data[col])
    print(col)
    print('Statistics=%.3f, p=%.3f' % (stat, p))
    # interpret
    alpha = 0.05
    if p > alpha:
        print('Sample looks Gaussian (fail to reject H0)')
        print('\n')
    else:
        print('Sample does not look Gaussian (reject H0)')
        print('\n')
    cnt +=1
MDEC-23
Statistics=0.986, p=0.075
Sample looks Gaussian (fail to reject H0)


MATS2v
Statistics=0.966, p=0.000
Sample does not look Gaussian (reject H0)


ATSC8s
Statistics=0.946, p=0.000
Sample does not look Gaussian (reject H0)


VE3_Dt
Statistics=0.559, p=0.000
Sample does not look Gaussian (reject H0)


CrippenMR
Statistics=0.762, p=0.000
Sample does not look Gaussian (reject H0)


SpMax7_Bhe
Statistics=0.714, p=0.000
Sample does not look Gaussian (reject H0)


SpMin1_Bhs
Statistics=0.968, p=0.001
Sample does not look Gaussian (reject H0)


C1SP2
Statistics=0.944, p=0.000
Sample does not look Gaussian (reject H0)


GATS8e
Statistics=0.802, p=0.000
Sample does not look Gaussian (reject H0)


GATS8s
Statistics=0.827, p=0.000
Sample does not look Gaussian (reject H0)


SpMax5_Bhv
Statistics=0.837, p=0.000
Sample does not look Gaussian (reject H0)


VE3_Dzi
Statistics=0.505, p=0.000
Sample does not look Gaussian (reject H0)


VPC-4
Statistics=0.924, p=0.000
Sample does not look Gaussian (reject H0)


df.head()
MDEC-23	MATS2v	ATSC8s	VE3_Dt	CrippenMR	SpMax7_Bhe	SpMin1_Bhs	C1SP2	GATS8e	GATS8s	SpMax5_Bhv	VE3_Dzi	VPC-4	Class
0	60.1757	-0.0231	-0.6667	-167.1241	0.0000	3.4009	2.3109	4	1.0229	1.0575	3.5545	-15.5940	4.1692	0
1	44.5031	-0.1236	-16.5096	-16.2080	172.2000	3.3611	2.1117	2	1.7155	1.7013	3.6066	-14.3317	2.0821	0
2	37.5488	0.0662	19.3467	-159.1796	173.4028	3.2705	2.0198	8	0.6992	0.7828	3.6441	-25.4493	2.8730	0
3	40.5929	0.0714	-9.5672	-21.4416	177.2726	3.2748	2.0191	6	0.9951	1.0298	3.6564	-19.6376	3.0444	0
4	52.7343	-0.0861	-11.8892	-2.0780	171.1315	3.4094	2.1664	2	0.7363	0.7427	3.5216	-8.2157	2.9469	0
Tukey
#Tukey Method

# Import required libraries
from collections import Counter

# Outlier detection 
def detect_outliers(df,n,features):
    
    outlier_indices = []
    
    # iterate over features(columns)
    for col in features:
        # 1st quartile (25%)
        Q1 = np.percentile(df[col], 25)
        # 3rd quartile (75%)
        Q3 = np.percentile(df[col],75)
        # Interquartile range (IQR)
        IQR = Q3 - Q1
        
        # outlier step
        outlier_step = 1.5 * IQR
        
        # Determine a list of indices of outliers for feature col
        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index
        
        # append the found outlier indices for col to the list of outlier indices 
        outlier_indices.extend(outlier_list_col)
        
    # select observations containing more than 2 outliers
    outlier_indices = Counter(outlier_indices)        
    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )
    
    return multiple_outliers   

# List of Outliers
Outliers_to_drop = detect_outliers(df.drop('Class',axis=1),0,list(df.drop('Class',axis=1)))
#df.drop('Class',axis=1).loc[Outliers_to_drop]
len(Outliers_to_drop)
61
#Create x and y variables
x = df.drop('Class', axis=1).to_numpy()
Y=df['Class'].to_numpy()

#Create Train and Test Dataset
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,Y,test_size = 0.2,stratify=Y,random_state = 100)
SMOTE
#Fix the imbalanced Classes
from imblearn.over_sampling import SMOTE
smt=SMOTE(random_state=100)
x_train_smt,y_train_smt = smt.fit_resample(x_train,y_train)

#Scale the Data
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train2 = sc.fit_transform(x_train_smt)
x_test2 = sc.transform(x_test)
#Class Balance - Test Data
print('Train Data - Class Split')
num_zeros = (y_train_smt == 0).sum()
num_ones = (y_train_smt == 1).sum()
print('Class 0 -',  num_zeros)
print('Class 1 -',  num_ones)
Train Data - Class Split
Class 0 - 91
Class 1 - 91
Key Findings from EDA
1 - No Missing Values: The dataset is complete with 171 entries. 2 - Class Imbalance: There are 115 "NonToxic" samples and 56 "Toxic" samples, showing an imbalance. 3 - Feature Correlations: The heatmap provides insights into how features are correlated.

Logistics Regression
# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(solver='lbfgs',class_weight='balanced',random_state = 0)
classifier.fit(x_train2, y_train_smt)

LogisticRegression
?i
LogisticRegression(class_weight='balanced', random_state=0)
# Predicting the Test set results
y_pred = classifier.predict(x_test2)
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, y_pred))
target_names=['Non-Toxic(0)','Toxic(1)']
print(classification_report(y_test, y_pred))
[[11 13]
 [ 5  6]]
              precision    recall  f1-score   support

           0       0.69      0.46      0.55        24
           1       0.32      0.55      0.40        11

    accuracy                           0.49        35
   macro avg       0.50      0.50      0.48        35
weighted avg       0.57      0.49      0.50        35

# Plot confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='crest', 
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix for Logistic Regression Model')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
No description has been provided for this image
#Construct some pipelines 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

#Create Pipeline

pipeline =[]

pipe_logreg = Pipeline([('scl', StandardScaler()),
                    ('clf', LogisticRegression(solver='lbfgs',class_weight='balanced',
                                               random_state=0))])
pipeline.insert(0,pipe_logreg)

# Set grid search params 

modelpara =[]

param_gridlogreg = {'clf__C': [0.01, 0.1, 1, 10, 100], 
                    'clf__penalty': ['l1', 'l2']}
modelpara.insert(0,param_gridlogreg)
#Define Plot for learning curve

from sklearn.model_selection import learning_curve

def plot_learning_curves(model):
    train_sizes, train_scores, test_scores = learning_curve(estimator=model,
                                                            X=x_train2, 
                                                            y=y_train_smt,
                                                            train_sizes= np.linspace(0.1, 1.0, 10),
                                                            cv=10,
                                                            scoring='recall_weighted',random_state=100)
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)
    
    plt.plot(train_sizes, train_mean,color='blue', marker='o', 
             markersize=5, label='training recall')
    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std,
                     alpha=0.15, color='blue')

    plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5,
             label='validation recall')
    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std,
                     alpha=0.15, color='green')
    plt.grid(True)
    plt.xlabel('Number of training samples')
    plt.ylabel('Recall')
    plt.legend(loc='best')
    plt.ylim([0.5, 1.01])
    plt.show()
#Plot Learning Curve
print('Logistic Regression Learning Curve')
plot_learning_curves(pipe_logreg)
Logistic Regression Learning Curve
No description has been provided for this image
Decision Trees
# for encoding
from sklearn.preprocessing import LabelEncoder
# for train test splitting
from sklearn.model_selection import train_test_split
# for checking testing results
from sklearn.metrics import classification_report, confusion_matrix
# for visualizing
from sklearn.tree import plot_tree
# for decision tree object
from sklearn.tree import DecisionTreeClassifier
# Defining the decision tree algorithm
dtree = DecisionTreeClassifier(random_state=0)
# fit the training data
dtree.fit(x_train2,y_train_smt)

DecisionTreeClassifier
?i
DecisionTreeClassifier(random_state=0)
# Testing
y_pred = dtree.predict(x_test2)
# Compute confusion matrix
cm = confusion_matrix(y_test,y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='summer', 
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix for Decision Tree Model')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
No description has been provided for this image
# Making the Confusion Matrix
from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test, y_pred))
[[19  5]
 [ 1 10]]
              precision    recall  f1-score   support

           0       0.95      0.79      0.86        24
           1       0.67      0.91      0.77        11

    accuracy                           0.83        35
   macro avg       0.81      0.85      0.82        35
weighted avg       0.86      0.83      0.83        35

Random Forest
# Importing the relevant Libraries 
import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import seaborn as sns
sns.set()
import warnings
warnings.filterwarnings('ignore')
from sklearn.ensemble import RandomForestClassifier 
classifier_rf = RandomForestClassifier(random_state=0) 
classifier_rf.fit(x_train2, y_train_smt)

RandomForestClassifier
?i
RandomForestClassifier(random_state=0)
# Testing
y_pred = classifier_rf.predict(x_test2)
#Print oob score
print(classifier_rf.oob_score)
False
# Compute confusion matrix
cm = confusion_matrix(y_test,y_pred)
# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='g', cmap='summer', 
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title('Confusion Matrix for Random Forest Model')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
No description has been provided for this image
#Making the Confusion Matrix
from sklearn.metrics import classification_report, confusion_matrix  
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test, y_pred))
[[20  4]
 [ 4  7]]
              precision    recall  f1-score   support

           0       0.83      0.83      0.83        24
           1       0.64      0.64      0.64        11

    accuracy                           0.77        35
   macro avg       0.73      0.73      0.73        35
weighted avg       0.77      0.77      0.77        35

#Construct some pipelines 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
rfecv = RFECV(
    estimator=DecisionTreeClassifier(random_state=0),
    step=1,
    cv=StratifiedKFold(5), 
    scoring='recall_weighted',  
    min_features_to_select=1,
)
#Create Pipeline

pipeline =[]

pipe_dt = Pipeline([('scl', StandardScaler()),('rfe',rfecv),
                    ('clf', DecisionTreeClassifier(random_state=0))])
pipeline.insert(0,pipe_dt)
#Define Plot for learning curve

from sklearn.model_selection import learning_curve

def plot_learning_curves(model):
    train_sizes, train_scores, test_scores = learning_curve(estimator=model,
                                                            X=x_train2, 
                                                            y=y_train_smt,
                                                            train_sizes= np.linspace(0.1, 1.0, 10),
                                                            cv=10,
                                                            scoring='recall_weighted',random_state=100)
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)
    
    plt.plot(train_sizes, train_mean,color='blue', marker='o', 
             markersize=5, label='training accuracy')
    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std,
                     alpha=0.15, color='blue')

    plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5,
             label='validation accuracy')
    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std,
                     alpha=0.15, color='green')
    plt.grid(True)
    plt.xlabel('Number of training samples')
    plt.ylabel('Recall')
    plt.legend(loc='best')
    plt.ylim([0.5, 1.01])
    plt.show()
#Plot Learning Curve
print('Decision Tree - Learning Curve')
plot_learning_curves(pipe_dt)
Decision Tree - Learning Curve
No description has been provided for this image
#Model Analysis
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score

models=[]
models.append(('Decision Tree',pipe_dt))


#Model Evaluation
results =[]
names=[]
scoring ='recall_weighted'
print('Model Evaluation - Recall')
for name, model in models:
    rkf=RepeatedKFold(n_splits=10, n_repeats=5, random_state=100)
    cv_results = cross_val_score(model,x_train2,y_train_smt,cv=rkf,scoring=scoring)
    results.append(cv_results)
    names.append(name)
    print('{} {:.2f} +/- {:.2f}'.format(name,cv_results.mean(),cv_results.std()))
print('\n')

#Boxpot View
fig = plt.figure(figsize=(10,5))
fig.suptitle('Boxplot View')
ax = fig.add_subplot(111)
sns.boxplot(data=results)
ax.set_xticklabels(names)
plt.ylabel('Recall')
plt.xlabel('Model')
plt.show()
Model Evaluation - Recall
Decision Tree 0.80 +/- 0.09


No description has been provided for this image
Feature Selection - RFE + Hyperparameter Tuning + Optimized Model
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import RFECV
from sklearn.model_selection import GridSearchCV
# Define RFECV
dt=DecisionTreeClassifier(random_state=0)
cv2=RepeatedKFold(n_splits=5, n_repeats=1, random_state=0)

rfecv = RFECV(
    estimator=dt,
    step=1,
    cv=StratifiedKFold(5), 
    scoring='recall_weighted',  
    min_features_to_select=1,
)
#RFECV Feature Selection
rfecv.fit(x_train2, y_train_smt)

print(f"Optimal number of features: {rfecv.n_features_}")

x_train2_selected = rfecv.transform(x_train2)
x_test2_selected = rfecv.transform(x_test2)

param_grid = {
    'max_depth': [100],
    'min_samples_split': [100],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
    'criterion': ['gini', 'entropy']
}
#GridSearch CV
gs_clf = GridSearchCV(dt, param_grid,cv=cv2,scoring='recall_weighted')
gs_clf = gs_clf.fit(x_train2_selected, y_train_smt)

# Get the best estimator (the Decision Tree with the best hyperparameters)
best_dt = gs_clf.best_estimator_ 
    
# Use best model and test data for final evaluation
y_pred = best_dt.predict(x_test2_selected)

#Identify Best Parameters to Optimize the Model
bestpara=str(gs_clf.best_params_)
    
#Output Heading
print('\nOptimized Model')
#Feature Importance - optimized
print('Feature Importances')
for name, score in zip(list(df),best_dt.feature_importances_):
    print(name, round(score,2))
    
#Output Validation Statistics
target_names=['Non-Toxic','Toxic']
print('\nBest Parameters:',bestpara)
print('\n', confusion_matrix(y_test,y_pred))  
print('\n',classification_report(y_test,y_pred,target_names=target_names))     
Optimal number of features: 11

Optimized Model
Feature Importances
MDEC-23 0.26
MATS2v 0.0
ATSC8s 0.09
VE3_Dt 0.17
CrippenMR 0.1
SpMax7_Bhe 0.12
SpMin1_Bhs 0.0
C1SP2 0.16
GATS8e 0.0
GATS8s 0.0
SpMax5_Bhv 0.1

Best Parameters: {'criterion': 'gini', 'max_depth': 100, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 100}

 [[18  6]
 [ 1 10]]

               precision    recall  f1-score   support

   Non-Toxic       0.95      0.75      0.84        24
       Toxic       0.62      0.91      0.74        11

    accuracy                           0.80        35
   macro avg       0.79      0.83      0.79        35
weighted avg       0.85      0.80      0.81        35

 
